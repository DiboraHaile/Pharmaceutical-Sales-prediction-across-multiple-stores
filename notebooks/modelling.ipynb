{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import sys  \n",
    "sys.path.insert(0, '../scripts')\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import clean_data\n",
    "import loading_data\n",
    "import utilities\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# load data\n",
    "df_store = loading_data.load_csv('../data/store.csv')\n",
    "df_train = loading_data.load_csv('../data/train.csv')\n",
    "df_test = loading_data.load_csv('../data/test.csv')\n",
    "df_submission = loading_data.load_csv('../data/sample_submission.csv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/dibora/Pharmaceutical-Sales-prediction-across-multiple-stores/pharmaceutical/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  \n",
      "/home/dibora/Pharmaceutical-Sales-prediction-across-multiple-stores/pharmaceutical/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/dibora/Pharmaceutical-Sales-prediction-across-multiple-stores/pharmaceutical/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  after removing the cwd from sys.path.\n",
      "/home/dibora/Pharmaceutical-Sales-prediction-across-multiple-stores/pharmaceutical/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# display some data from training and testing data\n",
    "### The testing data contains 8 columns, it doesn't include the sales because that is what we will be predicting.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df_train.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1017209 entries, 0 to 1017208\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   Store          1017209 non-null  int64 \n",
      " 1   DayOfWeek      1017209 non-null  int64 \n",
      " 2   Date           1017209 non-null  object\n",
      " 3   Sales          1017209 non-null  int64 \n",
      " 4   Customers      1017209 non-null  int64 \n",
      " 5   Open           1017209 non-null  int64 \n",
      " 6   Promo          1017209 non-null  int64 \n",
      " 7   StateHoliday   1017209 non-null  object\n",
      " 8   SchoolHoliday  1017209 non-null  int64 \n",
      "dtypes: int64(7), object(2)\n",
      "memory usage: 69.8+ MB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing Step\n",
    "### Process the data into a format where it can be fed to a machine learning model. This typically means converting all non-numeric columns to numeric, handling NaN values and generating new features from already existing features. \n",
    "\n",
    "### In our case, you have a few datetime columns to preprocess. you can extract the following from them:\n",
    "- weekdays\n",
    "- weekends \n",
    "- number of days to holidays\n",
    "- Number of days after holiday\n",
    "- Beginning of month, mid month and ending of month\n",
    "(think of more features to extract), extra marks for it\n",
    "\t\t\t\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# converting columns to numerical data types\n",
    "# Encoding state holiday values  \n",
    "df_train = utilities.format_datetime(df_train,\"Date\")\n",
    "df_test = utilities.format_datetime(df_test,\"Date\")\n",
    "\n",
    "# extracting numerical information from the date columns\n",
    "# the year\n",
    "df_train_copy = df_train.copy()\n",
    "df_train_copy[\"Year\"] = df_train_copy['Date'].dt.year\n",
    "# which part of the month it is where 0 is begining, 1 is mid and 2 is end\n",
    "df_train_copy[\"Part of the month\"] = df_train_copy['Date'].dt.day.apply(lambda x: x // 10)\n",
    "df_train_copy.loc[(df_train_copy[\"Date\"].dt.day == 31), \"Part of the month\"] = 2\n",
    "# How many days before or after holidays\n",
    "\n",
    "# Encoding holiday values to numerical ones\n",
    "holidays = {'0': 0, 'a': 1, 'b':2, 'c':3}\n",
    "df_train_copy[\"StateHoliday\"] = df_train_copy['StateHoliday'].map(lambda x: holidays[x])\n",
    "df_train_copy = df_train_copy.drop(columns=[\"Date\"])\n",
    "df_train_copy.info()\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1017209 entries, 0 to 1017208\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count    Dtype\n",
      "---  ------             --------------    -----\n",
      " 0   Store              1017209 non-null  int64\n",
      " 1   DayOfWeek          1017209 non-null  int64\n",
      " 2   Sales              1017209 non-null  int64\n",
      " 3   Customers          1017209 non-null  int64\n",
      " 4   Open               1017209 non-null  int64\n",
      " 5   Promo              1017209 non-null  int64\n",
      " 6   StateHoliday       1017209 non-null  int64\n",
      " 7   SchoolHoliday      1017209 non-null  int64\n",
      " 8   Year               1017209 non-null  int64\n",
      " 9   Part of the month  1017209 non-null  int64\n",
      "dtypes: int64(10)\n",
      "memory usage: 77.6 MB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Now that we have all our data converted to numerical values, We need to take out sales as our target variable and the others as our features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# preparing our features\n",
    "y_train = np.array(df_train_copy['Sales']).reshape(-1, 1) \n",
    "Store_ids = list(df_train_copy['Store'])\n",
    "X_train = np.array(df_train_copy.drop(columns=['Sales','Store']))\n",
    "\n",
    "y_train.shape, X_train.shape\n",
    "# X_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1017209, 1), (1017209, 8))"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### As a final thing, we have to scale the data. This helps with predictions especially when using machine learning algorithms that use Euclidean distances. you can use the standard scaler in sklearn for this.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# using standard scalar to scale our data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.50148416, -0.16826876,  0.45239852, ...,  2.14421115,\n",
       "         1.50207687,  1.03142726],\n",
       "       [ 0.50148416, -0.01754036,  0.45239852, ...,  2.14421115,\n",
       "         1.50207687,  1.03142726],\n",
       "       [ 0.50148416,  0.40449914,  0.45239852, ...,  2.14421115,\n",
       "         1.50207687,  1.03142726],\n",
       "       ...,\n",
       "       [-1.00047591, -1.36332959, -2.21044047, ...,  2.14421115,\n",
       "        -1.07061593, -1.28096673],\n",
       "       [-1.00047591, -1.36332959, -2.21044047, ...,  2.14421115,\n",
       "        -1.07061593, -1.28096673],\n",
       "       [-1.00047591, -1.36332959, -2.21044047, ...,  2.14421115,\n",
       "        -1.07061593, -1.28096673]])"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A reasonable starting point will be to use any of the tree based algorithms. Random forests Regressor will make for a good start. Before training our random forest model, we will transform, encode and scale our data. We will use "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "clf_decisions = tree.DecisionTreeClassifier()\n",
    "decision_clf_trained = clf_decisions.fit(X_train[:100000,],y_train[:100000])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 6.66666667e-01, 7.51218192e-02, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [8.97666068e-04, 6.66666667e-01, 8.45966432e-02, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [1.79533214e-03, 6.66666667e-01, 1.11126151e-01, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       ...,\n",
       "       [9.98204668e-01, 1.66666667e-01, 0.00000000e+00, ...,\n",
       "        3.33333333e-01, 1.00000000e+00, 0.00000000e+00],\n",
       "       [9.99102334e-01, 1.66666667e-01, 0.00000000e+00, ...,\n",
       "        3.33333333e-01, 1.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 1.66666667e-01, 0.00000000e+00, ...,\n",
       "        3.33333333e-01, 1.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now that we have scaled our data, we will "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('pharmaceutical': venv)"
  },
  "interpreter": {
   "hash": "3111f4c5a366be439e2804dc4d42d279c7960b52b896a3ee11ceabeb285b6b48"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}